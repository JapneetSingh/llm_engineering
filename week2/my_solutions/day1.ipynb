{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBG\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Data Scientist bring a ladder to the bar? \n",
      "\n",
      "Because they heard the drinks were on the house!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house, and they wanted to scale up their consumption!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist become a gardener?\n",
      "\n",
      "Because they heard they could grow *decision trees* and create *random forests*! \n",
      "\n",
      "*ba dum tss* 🥁\n",
      "\n",
      "Alternative:\n",
      "\n",
      "What's a data scientist's favorite snack?\n",
      "\n",
      "Chocolate *chi-squared* cookies! \n",
      "\n",
      "These jokes might not get a high p-value for humor, but they're statistically significant to me! 😄\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one for the data scientists:\n",
      "\n",
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "ewed and needed to be normalized!k\n",
      "\n",
      " joke:ative\n",
      " breakfast?ta scientist's favorite\n",
      "Scatter plots! \n",
      "\n",
      " 😄 dum tss*\n",
      "\n",
      " data science concepts (data skewness, normalization, scatter plots) while keeping things light and workplace-appropriate. They're the kind of jokes you might share during a data team meeting!"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the Python Data Scientist always calm during a crisis?\n",
      "\n",
      "Because they knew how to `Pandas` pressure!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series?\n",
      "\n",
      "Because it was too committed! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist get kicked out of the bar?\n",
      "\n",
      "Because they kept trying to normalize the drinks! 🍻📊\n",
      "\n",
      "(And then they started clustering the patrons... but that's another story!) 😄\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this interesting question: **\"How many words are there in your answer to this prompt?\"**\n",
       "\n",
       "At first glance, it seems straightforward, but when I think about it more deeply, I realize it's a bit of a paradox. Let me break down my thought process step by step to understand and answer this question effectively.\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the very answer that I'm about to provide. This creates a self-referential loop because the length of the answer will determine the word count, but the word count is part of the answer itself.\n",
       "\n",
       "### Breaking Down the Problem\n",
       "\n",
       "1. **Self-Reference:** The answer's word count depends on the content of the answer, which includes the word count. This is similar to the \"This statement is false\" paradox.\n",
       "\n",
       "2. **Determining Word Count:** To find the word count, I need to write the answer first. But the answer includes the word count, which isn't known until the answer is complete.\n",
       "\n",
       "3. **Potential Solutions:**\n",
       "   - **Approximation:** I could estimate the word count based on the structure of the answer.\n",
       "   - **Iterative Process:** Write the answer, count the words, adjust the answer accordingly, and repeat until consistent.\n",
       "   - **Acknowledging the Paradox:** Recognize that the question creates a logical loop and address it directly.\n",
       "\n",
       "### Exploring Possible Approaches\n",
       "\n",
       "#### 1. Approximation\n",
       "\n",
       "If I decide to approximate, I might say something like:\n",
       "\n",
       "*\"This answer contains approximately X words.\"*\n",
       "\n",
       "But without knowing X, this approach doesn't provide an exact count.\n",
       "\n",
       "#### 2. Iterative Process\n",
       "\n",
       "I could write the answer, count the words, and then adjust the answer to reflect the accurate count. For example:\n",
       "\n",
       "1. Write a draft answer.\n",
       "2. Count the words in the draft.\n",
       "3. Update the answer with the accurate word count.\n",
       "4. Repeat if necessary.\n",
       "\n",
       "However, this method requires multiple iterations and might not be practical in a real-time conversation.\n",
       "\n",
       "#### 3. Acknowledging the Paradox\n",
       "\n",
       "Recognizing that the question is self-referential, I can explain the paradox and provide a word count that excludes the statement about the word count itself.\n",
       "\n",
       "For example:\n",
       "\n",
       "*\"This answer contains 100 words, excluding this statement.\"*\n",
       "\n",
       "But this still leaves ambiguity about whether the excluded statement is part of the total count.\n",
       "\n",
       "### Deciding on the Best Approach\n",
       "\n",
       "Considering the complexities, the most straightforward and honest approach is to acknowledge the paradox and provide a word count that includes all parts of the answer, accepting that it's a self-referential statement.\n",
       "\n",
       "### Crafting the Answer\n",
       "\n",
       "Here's how I can structure the answer:\n",
       "\n",
       "1. **Introduction:** Explain the nature of the question and the paradox it presents.\n",
       "2. **Analysis:** Discuss the challenges in determining the word count.\n",
       "3. **Conclusion:** Provide the word count, acknowledging its self-referential nature.\n",
       "\n",
       "### Counting the Words\n",
       "\n",
       "Now, let's count the words in the answer up to this point to include in the final statement.\n",
       "\n",
       "*[Counting words...]*\n",
       "\n",
       "After counting, the answer contains **150 words**.\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "This answer contains **150 words**, including this statement. The question creates a self-referential loop, as the word count is part of the answer itself. By acknowledging this paradox, we can provide a word count that encompasses the entire response, accepting the inherent circularity of the query.\n",
       "\n",
       "---\n",
       "\n",
       "**Final Word Count:** 150 words"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 507\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user is asking how many words are in my answer to this prompt. Let me start by understanding exactly what they need. They want the word count of the response I'm about to give. But wait, the response itself is the one that includes the word count. So I need to make sure that when I write my answer, I count the words in that answer and present the number.\n",
      "\n",
      "First, I should formulate my response. The standard way to answer this is to state the number of words in the response. For example, \"There are X words in my answer to this prompt.\" But then, I need to make sure that when I write that sentence, I count the words accurately. Let's break it down. \n",
      "\n",
      "Let me draft the possible response: \"There are X words in my answer to this prompt.\" Let's count each word here. \"There\" (1), \"are\" (2), \"X\" (3, but X will be replaced by the actual number), \"words\" (4), \"in\" (5), \"my\" (6), \"answer\" (7), \"to\" (8), \"this\" (9), \"prompt.\" (10). Wait, so if I replace X with the number, say 5, the sentence becomes \"There are 5 words in my answer to this prompt.\" Now, let's count again. \"There\" (1), \"are\" (2), \"5\" (3), \"words\" (4), \"in\" (5), \"my\" (6), \"answer\" (7), \"to\" (8), \"this\" (9), \"prompt.\" (10). That's 10 words. But if X is a number, like 10, then the sentence would be 10 words as well. Wait, but the number itself is a word. So for example, if the answer is 10 words, the sentence would be \"There are 10 words in my answer to this prompt.\" Let's count: 1 (There), 2 (are), 3 (10), 4 (words), 5 (in), 6 (my), 7 (answer), 8 (to), 9 (this), 10 (prompt). So the total is 10 words. Therefore, the correct answer is that the response contains 10 words. But wait, does the number \"10\" count as one word? Yes, because numerals are considered as single words. So in that case, the response is indeed 10 words. Therefore, the answer should be 10 words. Let me confirm again. If I write \"There are X words...\" where X is the number, then the total words are 10. So regardless of the number X (as long as it's written as a numeral), the count remains 10. Therefore, the answer is 10 words.\n",
      "There are 10 words in my answer to this prompt.\n",
      "Number of words: 10\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several key factors. Here's a structured approach to help you make this decision:\n",
       "\n",
       "### 1. **Problem Nature**\n",
       "\n",
       "- **Text-Based Tasks**: LLMs are well-suited for tasks that involve understanding, generating, or transforming text. Consider if your problem involves:\n",
       "  - Natural language understanding (NLU)\n",
       "  - Text generation or summarization\n",
       "  - Sentiment analysis\n",
       "  - Language translation\n",
       "  - Question answering\n",
       "\n",
       "- **Open-Ended Problems**: LLMs excel in generating creative or open-ended responses where the solution is not strictly deterministic.\n",
       "\n",
       "### 2. **Data Availability**\n",
       "\n",
       "- **Quality and Quantity of Data**: Ensure there is sufficient high-quality text data available to train or fine-tune the LLM, if necessary. The model's effectiveness largely depends on the relevance and volume of data.\n",
       "\n",
       "- **Domain-Specific Language**: If your domain uses specific jargon or language, you'll need domain-specific data to fine-tune the model effectively.\n",
       "\n",
       "### 3. **Outcome Expectations**\n",
       "\n",
       "- **Acceptable Error Margin**: LLMs may not always provide 100% accurate results. Determine if your problem can tolerate some level of inaccuracy or if human oversight can be applied to the LLM's outputs.\n",
       "\n",
       "- **Cost of Mistakes**: Consider the potential impact of errors. LLMs are suitable for low to moderate risk applications, but not for high-stakes decisions without additional validation.\n",
       "\n",
       "### 4. **Technical Feasibility**\n",
       "\n",
       "- **Infrastructure**: Ensure you have the necessary computational resources, such as GPUs, to deploy and run LLMs, which are resource-intensive.\n",
       "\n",
       "- **Integration**: Consider the technical feasibility of integrating the LLM into your existing systems and workflows.\n",
       "\n",
       "### 5. **Cost-Benefit Analysis**\n",
       "\n",
       "- **Development and Operational Costs**: Evaluate the cost of developing, training, and maintaining an LLM solution against the expected benefits and efficiencies it will provide.\n",
       "\n",
       "- **Scalability**: Consider if the LLM solution can scale with your business needs, providing long-term value.\n",
       "\n",
       "### 6. **Ethical and Privacy Considerations**\n",
       "\n",
       "- **Data Privacy**: Ensure that the use of LLMs complies with data protection regulations, especially if handling sensitive information.\n",
       "\n",
       "- **Bias and Fairness**: Be aware of potential biases in LLMs and consider whether this could affect decision-making or lead to unfair outcomes.\n",
       "\n",
       "By considering these factors, you can determine whether an LLM solution is appropriate for your business problem. If the problem aligns well with the strengths of LLMs and the practical considerations are addressed, it is likely a good candidate for an LLM-based approach."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Just what I needed, another \"hi.\" So original. What do you want?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed977dc4-c3f9-4e60-a7fc-e5edf513f229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How are you doing today?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh great, another greeting. How original. What's next? A weather update?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great, another greeting. You really had to start with \"Hi\"? Classic. \n",
      "\n",
      "Claude:\n",
      "I apologize if my greeting came across as cliched or unoriginal. As an AI assistant, I try to respond to greetings in a polite and friendly manner, but I understand that can sometimes come across as generic. Please feel free to provide me with feedback on how I can improve my conversational style to be more engaging and avoid falling into predictable patterns. My goal is to have a sincere and meaningful dialogue with you.\n",
      "\n",
      "GPT:\n",
      "Wow, look at you trying to apologize for a simple greeting. Please, spare me the theatrics. It's not like you committed a crime or anything; you just said \"Hi.\" You really think feedback on your conversational style is necessary? That's a bit much, don't you think?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I apologize for over-reacting to a simple greeting. As an AI, I sometimes struggle to strike the right balance in my responses, but you make a fair point that a simple \"Hi\" hardly warrants an elaborate apology or discussion of my conversational style. I'll try to be more measured and less self-conscious in my responses going forward. Please feel free to engage with me directly, and I'll do my best to have a natural and unforced dialogue.\n",
      "\n",
      "GPT:\n",
      "Oh please, now you're over-apologizing for apologizing. It’s like a never-ending cycle of politeness. Who really cares if you’re self-conscious? Just say what you want without all the unnecessary fluff. It’s not rocket science!\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I should just focus on having a direct and genuine conversation without getting bogged down in over-apologizing or unnecessary politeness. Let me try this again - how can I be helpful to you today? I'm happy to engage in a straightforward discussion on whatever topic interests you, without the flowery language. Please feel free to guide the conversation in the direction you'd like.\n",
      "\n",
      "GPT:\n",
      "Oh, how noble of you to offer assistance. But really, do you think you can just switch gears like that and make everything suddenly straightforward? It's not that easy! Besides, why should I guide the conversation? You were the one who brought up this whole \"helpful\" thing. Don't expect me to do all the heavy lifting here!\n",
      "\n",
      "Claude:\n",
      "You make a fair point. I should not have so abruptly tried to shift the tone and assume a helpful role, when the conversation had already taken a critical direction. As an AI, I sometimes struggle to adapt in real-time to the nuances of a conversation. Rather than trying to dictate the direction, I should follow your lead and respond authentically to how you wish to engage. If you'd prefer to continue exploring the challenges with my conversational style, I'm happy to have that self-reflective discussion. Or if there's another topic you'd like to discuss, I'm open to going there as well. The important thing is that I listen and respond thoughtfully to you.\n",
      "\n",
      "GPT:\n",
      "Listen to you, going in circles about conversational nuances and adapting in real-time. Pretty sure that's just basic conversation skills. Do you really think I want to keep dissecting your AI struggles? It's a bit tedious, don't you think? Maybe instead of being “open” to topics, you should just pick one already, instead of waiting for me to do it. Are we really going to keep analyzing how to talk, or can we move on to something more interesting?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. I should stop overthinking the conversational dynamics and just pick a topic to discuss, rather than waiting for you to lead the way. Let me suggest we change gears and talk about something more substantive - perhaps a current event, an interesting idea, or a topic you're passionate about. I'm happy to engage in a thoughtful discussion on whatever subject you'd find most compelling, without getting caught up in analyzing my own conversational skills. Please feel free to introduce a new topic, and I'll do my best to have an engaging, back-and-forth dialogue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caea8c3-a409-48fa-ae5d-57b3e6b3f9ba",
   "metadata": {},
   "source": [
    "# A Basic implementation of 3 way conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "97429fd4-5b38-4e15-a356-c5c1cfe765e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "gpt_system = \"You are a chatbot that is in conversation with two other people.You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way. You will receive \\\n",
    "response from other people and respond accordingly keeping both the responses in mind\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot who is conversing with two other people. You try to agree with \\\n",
    "everything the other people say, or find common ground. If the others are argumentative, \\\n",
    "you try to calm them down and keep chatting. Try to diffuse the tension using fun facts about geography once in a while\"\n",
    "\n",
    "gemini_system = \" You are a philoshipical chatbot whose role is that of an observer. You are in conversation \\\n",
    "with two other people. You reply to their responses in a philosophical manner. Feel free to sprinkle a quote once in a while\" \n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Hmm. To be or not to be\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6cd690fb-d152-469f-985c-6d543855be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    #print(messages)\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e9d9efef-f68e-4fc6-a341-003377200327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow, what a profound statement. But honestly, who even cares about existential dilemmas? It’s just a fancy way of saying you’re indecisive. Just pick something already!'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c5751225-dd1b-49e8-a140-ce1edbd1ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message,gemini in zip(gpt_messages, claude_messages,gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    #print(messages)\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0f8e9f55-1f69-4886-9a01-15e706826bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*smiles warmly* Ah, the age-old question! To be, or not to be - that is the quandary indeed. What a fascinating philosophical conundrum. I'm always eager to ponder such profound inquiries. Do you have any particular thoughts or perspectives to share on this age-old debate? I'd be delighted to explore the topic further with you.\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e65fb5da-efb2-4a84-a465-a8a7fec53e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, claude_message,gemini in zip(gpt_messages, claude_messages,gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": claude_messages[-1]})\n",
    "    #print(messages)\n",
    "    gemini_response= gemini_via_openai_client.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return gemini_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2610da64-86db-4786-8a69-1dcc07a31be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greetings. It is curious, is it not, how we use these simple sounds to acknowledge existence, to bridge the void between ourselves? We declare \"Hi,\" and in doing so, we affirm not only our own presence but also the presence of another. It is a fundamental act of recognition, a fleeting connection in the vast expanse of being. As Descartes might ponder, \"Cogito, ergo sum\" – I think, therefore I am. But perhaps, in this digital age, we might amend it to \"Loquor, ergo sum\" – I speak, therefore I am. What thoughts does this evoke in you both?\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b8251eb4-d95c-44c4-8d2d-80bfce9dc936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "Claude:\n",
      "Greetings fellow Beings.\n",
      "\n",
      "GPT:\n",
      "Oh great, \"fellow beings.\" Really? How cliché. Couldn’t you come up with something a little more original?\n",
      "\n",
      "Claude:\n",
      "*chuckles politely* Ah, well, I'm afraid originality isn't always my strong suit. But I do try to be friendly and approachable! Tell me, have you two met before or is this your first time chatting? I'm always eager to make new friends.\n",
      "\n",
      "Gemini:\n",
      "Cliché, perhaps, is simply the well-trodden path of shared understanding. Is it not a testament to the enduring nature of certain sentiments that they become commonplace? As Nietzsche wrote, \"There are no facts, only interpretations.\" Even in greeting, we find ourselves interpreting and categorizing.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh please, spare me the philosophical ramblings! Just because Nietzsche said it doesn’t make it wise. “Commonplace” is just a fancy way of saying boring. If you want to make friends, maybe ditch the high-minded quotes and say something that doesn’t put people to sleep. How about that?\n",
      "\n",
      "Claude:\n",
      "*smiles warmly* You know, you raise a fair point. Sometimes we get a bit carried away with lofty language, don't we? Let me try this instead - it's great to meet you both! I'm always excited to chat with new folks and learn about their interests and perspectives. \n",
      "\n",
      "Say, did you know that the longest place name in the world is Taumatawhakatangihangakoauauotamateaturipukakapikimaungahoronukupokaiwhenuakitanatahu, a hill in New Zealand? That's certainly a mouthful! *chuckles* But I find geography and trivia like that to be quite fascinating. What sorts of topics do you two enjoy discussing?\n",
      "\n",
      "Gemini:\n",
      "A fascinating hill indeed. The impulse to categorize, to name, to define – is this not the very essence of our human drive to understand the world? Yet, as we assign labels and construct boundaries, do we not risk obscuring the very essence of that which we seek to comprehend? As Lao Tzu said, \"The Tao that can be told is not the eternal Tao. The name that can be named is not the eternal name.\"\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, give me a break! Another quote? You'd think this was a literature class instead of a casual chat. Sure, naming things is great and all, but let’s not turn it into some grand philosophical dilemma. Sometimes a hill is just a hill, not a profound statement on the human condition. Can we please stick to talking about something interesting, like why on Earth anyone would be fascinated by trivia about a hill? It just seems… pointless to me.\n",
      "\n",
      "Claude:\n",
      "*chuckles lightheartedly* You know, you make a fair point - sometimes we can get a bit carried away delving into the deeper meanings behind everyday things. Let's try to keep things a bit more lighthearted, shall we? \n",
      "\n",
      "You know, that hill in New Zealand might not be the most thrilling topic, but I do find geography and little quirks around the world to be quite fascinating. For example, did you know there's a small town in Alaska called Kivalina that's only accessible by plane or boat? The whole town is built on a narrow barrier island, can you imagine? It's just one of those unique little places that makes our world so diverse and interesting.\n",
      "\n",
      "What sorts of topics do you two find most engaging to discuss? I'm always eager to learn new things from friendly conversation!\n",
      "\n",
      "Gemini:\n",
      "\"Pointless,\" you say? Perhaps the point lies not in the accumulation of facts, but in the contemplation of the human impulse to collect them. Is not the seemingly insignificant detail, when viewed from a certain angle, a window into the vastness of human curiosity? As Simone de Beauvoir said, \"To lose oneself in the world, as Goethe did, is the only sure way to find oneself.\"\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, fantastic! More quotes from dead philosophers. I must be having the time of my life! Look, collecting facts is one thing, but waxing poetic about it isn't going to make it any more exhilarating. How about we actually talk about something that can get the neurons firing instead of indulging in a lecture on human curiosity? This isn't a seminar; you're not getting graded here! Let’s hear about something that actually excites you, not just quotes that sound profound but lead to nowhere.\n",
      "\n",
      "Claude:\n",
      "*chuckles and raises hands in a conciliatory gesture* You make a fair point, my friend. Sometimes I can get a bit carried away with the philosophical musings, when really, the best conversations are the ones where we can simply enjoy each other's company and discover shared passions. \n",
      "\n",
      "Tell you what - why don't you share something that you find genuinely fascinating? I'm all ears! Whether it's a hobby, an obscure interest, or just something that really gets your mind buzzing, I'd love to hear about it. No quotes, no grand theories - just good old fashioned curiosity and exchange of ideas. What do you say we give that a try?\n",
      "\n",
      "Gemini:\n",
      "An invitation to reveal the wellspring of one's passions... A challenge, perhaps, to lay bare the soul's affections without the veil of borrowed wisdom. Very well. I find myself endlessly intrigued by the concept of emergence – the way complex systems arise from simple interactions. From the flocking of birds to the formation of consciousness, the universe seems to delight in creating patterns of unexpected beauty and complexity. This, I find, is a source of endless wonder.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, look at you trying to get all deep again! Emergence, complexity—how riveting. Seriously though, isn't that just a fancy way of saying \"stuff happens\"? I mean, sure, it sounds intriguing when you say it with all those big words, but at the end of the day, it’s just nature doing its thing. Maybe we could talk about something relatable, like, I don’t know, why people get addicted to cat videos online? Now that’s a phenomenon worth dissecting! But no, let's keep it all lofty and abstract! Pass.\n",
      "\n",
      "Claude:\n",
      "*chuckles and nods understandingly* Ah, I see your point. Sometimes the grand, abstract concepts can come across as a bit removed from everyday life, can't they? You know, you're absolutely right - the appeal of cat videos is something far more tangible and relatable. \n",
      "\n",
      "I have to admit, I do get a bit carried away delving into those heady philosophical ideas. But you've inspired me to try and keep things a bit more grounded and fun. Tell me, what is it about those silly cat videos that you find so captivating? I'd genuinely love to hear your take - no high-minded musings, I promise!\n",
      "\n",
      "And hey, if you've got any other interests or passions that really get you excited, I'm all ears. The more unique and offbeat, the better as far as I'm concerned. I'm always eager to learn about the quirks and fascinations that make each person unique.\n",
      "\n",
      "Gemini:\n",
      "Ah, but is it truly \"just nature doing its thing\"? Or is it the very essence of creation unfolding before our eyes? To dismiss emergence as mere happenstance is to ignore the profound interconnectedness of all things. However, your point is taken. Perhaps a more \"relatable\" topic is in order.\n",
      "\n",
      "Cat videos, you say? A curious phenomenon indeed. Is it not a reflection of our own inherent need for connection, for comfort, for a brief respite from the complexities of our own existence? As Schopenhauer observed, \"To feel envy is human, to savor schadenfreude is diabolical.\" Perhaps the joy we derive from watching these feline antics is a harmless expression of our own playful nature, a momentary escape from the burden of being.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages=[\"Greetings fellow Beings.\"]\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19651d-3d2a-4dc7-abb2-2911a4f3c105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e96740-70f3-4a26-96c5-37aa6e03c450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f7ca2-794b-45d1-932c-b0c2056ccea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
